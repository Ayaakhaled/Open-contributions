Predicting Credit Card Approvals
#!/usr/bin/env python
# coding: utf-8

# ## Machine learning 
# Machine Learning is a sub-area of artificial intelligence, whereby the term refers to the ability of IT systems to independently find solutions to problems by recognizing patterns in databases. In other words: Machine Learning enables IT systems to recognize patterns on the basis of existing algorithms and data sets and to develop adequate solution concepts. Therefore, in Machine Learning, artificial knowledge is generated on the basis of experience.

# __Applications of ML__
# 
# the following tasks by Machine Learning:
# 
# - Finding, extracting and summarizing relevant data
# - Making predictions based on the analysis data
# - Calculating probabilities for specific results
# - Adapting to certain developments autonomously
# - Optimizing processes based on recognized patterns

# ## The key learning points from module 2 : 

# - Definitions of ML 
# - types of ML 
# - how we can use each type 

# # in the following
# 
# i tried to apply what i 've been learning this week , i tried to solve the problem of Predicting Credit Card Approvals

# In[3]:


import pandas as pd 


# In[4]:


df = 'cc_approvals.csv'


# In[5]:


df1 = pd.read_csv(df)


# In[6]:


df1.head()


# In[7]:


# Print summary statistics
cc_apps_description = df1.describe()
print(cc_apps_description)

print("\n")

# Print DataFrame information
cc_apps_info = df1.info()
print(cc_apps_info)

print("\n")

# Inspect missing values in the dataset
df1.tail(17)


# In[8]:


# Import numpy
import numpy as np

# Inspect missing values in the dataset
print(df1.tail(17))

# Replace the '?'s with NaN
cc_apps = df1.replace('?', np.nan)

# Inspect the missing values again
cc_apps.tail(17)


# In[11]:


# Impute the missing values with mean imputation
df1.fillna(df1.mean(), inplace=True)

# Count the number of NaNs in the dataset and print the counts to verify
print(df1.isnull().sum())


# In[13]:


# Iterate over each column of cc_apps
for col in df1.columns:
    # Check if the column is of object type
    if df1[col].dtypes == 'object':
        # Impute with the most frequent value
        df1 = df1.fillna(df1[col].value_counts().index[0])

# Count the number of NaNs in the dataset and print the counts to verify
print(df1.isnull().sum())


# In[17]:


cc_apps = pd.read_csv(df)
cc_apps.head()


# In[25]:


# Import LabelEncoder
from sklearn.preprocessing import LabelEncoder

# Instantiate LabelEncoder
le=LabelEncoder()

# Iterate over all the values of each column and extract their dtypes
for col in cc_apps.columns.values:
    # Compare if the dtype is object
    if cc_apps[col].dtypes=='object':
    # Use LabelEncoder to do the numeric transformation
        cc_apps[col]=le.fit_transform(cc_apps[col])


# In[ ]:



# Import train_test_split
from sklearn.model_selection import train_test_split

# Drop the features 11 and 13 and convert the DataFrame to a NumPy array
cc_apps = cc_apps.drop([11, 13], axis=1)
cc_apps = cc_apps.values

# Segregate features and labels into separate variables
X,y = cc_apps[:,0:13] , cc_apps[:,13]

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X
                                                    ,
                                                    y,
                                                    test_size=0.33,
                                                    random_state=42)


# In[ ]:


# Import MinMaxScaler
from sklearn.preprocessing import MinMaxScaler

# Instantiate MinMaxScaler and use it to rescale X_train and X_test
scaler = MinMaxScaler(feature_range=(0, 1))
rescaledX_train = scaler.fit_transform(X_train)
rescaledX_test = scaler.transform(X_test)


# In[ ]:


# Import LogisticRegression
from sklearn.linear_model import LogisticRegression

# Instantiate a LogisticRegression classifier with default parameter values
logreg = LogisticRegression()

# Fit logreg to the train set
logreg.fit(rescaledX_train,y_train)


# In[ ]:


# Import confusion_matrix
from sklearn.metrics import confusion_matrix

# Use logreg to predict instances from the test set and store it
y_pred = logreg.predict(rescaledX_test)

# Get the accuracy score of logreg model and print it
print("Accuracy of logistic regression classifier: ", logreg.score(rescaledX_test,y_test))

# Print the confusion matrix of the logreg model
confusion_matrix(y_test,y_pred)


# In[ ]:


# Import GridSearchCV
from sklearn.model_selection import GridSearchCV

# Define the grid of values for tol and max_iter
tol = [0.01, 0.001 ,0.0001]
max_iter = [100, 150, 200]

# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values
param_grid = dict(tol=tol, max_iter=max_iter)


# In[ ]:


# Instantiate GridSearchCV with the required parameters
grid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)

# Use scaler to rescale X and assign it to rescaledX
rescaledX = scaler.fit_transform(X)

# Fit grid_model to the data
grid_model_result = grid_model.fit(rescaledX, y)

# Summarize results
best_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_
print("Best: %f using %s" % (best_score, best_params))


# In[ ]:




